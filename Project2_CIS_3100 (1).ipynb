{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Q1JtbaFyKBF2"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import csv\n",
        "from random import shuffle\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataHandler:\n",
        "    def __init__(self, filepath):\n",
        "        # Constructor to initialize the filepath\n",
        "        self.filepath = filepath\n",
        "\n",
        "    def read_csv(self):\n",
        "        # Read data from a CSV file and store it in a list\n",
        "        with open(self.filepath, 'r') as file:\n",
        "            csv_reader = csv.reader(file)\n",
        "            next(csv_reader)  # Skip the header row\n",
        "            dataset = [row for row in csv_reader]\n",
        "        return dataset\n",
        "\n",
        "    def train_test_split(self, dataset, test_size=0.2):\n",
        "        # Shuffle the dataset to ensure randomness\n",
        "        shuffle(dataset)\n",
        "        # Determine the split index based on the test size\n",
        "        split_index = int(len(dataset) * (1 - test_size))\n",
        "        # Split the dataset into training and testing sets\n",
        "        return dataset[:split_index], dataset[split_index:]\n",
        "\n",
        "\n",
        "    def separate_features_labels(self, dataset):\n",
        "        # Separate the features and labels from the dataset\n",
        "        # Convert the feature values to floats for computation\n",
        "        features = [list(map(float, data[1:-1])) for data in dataset]  # Exclude the ID and label\n",
        "        labels = [data[-1] for data in dataset]  # The label is the last element in each row\n",
        "        return features, labels"
      ],
      "metadata": {
        "id": "6TxGm2gNLgSI"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NaiveBayesClassifier:\n",
        "    def __init__(self):\n",
        "        # Initialize dictionaries to store the means, standard deviations,\n",
        "        # and class probabilities for each class\n",
        "        self.means = {}\n",
        "        self.stds = {}\n",
        "        self.class_probabilities = {}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Train the classifier by calculating the class probabilities\n",
        "        # and the means and standard deviations for each feature\n",
        "        self._calculate_class_probabilities(y)\n",
        "        self._calculate_means_stds(X, y)\n",
        "\n",
        "    def _calculate_class_probabilities(self, y):\n",
        "        # Calculate the probability of each class based on label frequency\n",
        "        class_counts = {label: y.count(label) for label in set(y)}\n",
        "        total_count = len(y)\n",
        "        self.class_probabilities = {label: count / total_count for label, count in class_counts.items()}\n",
        "\n",
        "    def _calculate_means_stds(self, X, y):\n",
        "        # Calculate the mean and standard deviation for each class and each feature\n",
        "        for label in self.class_probabilities:\n",
        "            # Extract features for instances of the current class\n",
        "            label_features = [X[i] for i in range(len(X)) if y[i] == label]\n",
        "            # Calculate mean and standard deviation for each feature\n",
        "            self.means[label] = [sum(f) / len(f) for f in zip(*label_features)]\n",
        "            self.stds[label] = [math.sqrt(sum([(x - mean)**2 for x in f]) / len(f)) for mean, f in zip(self.means[label], zip(*label_features))]\n",
        "\n",
        "    def predict_single(self, input_features):\n",
        "        # Predict the class of a single feature set\n",
        "        probabilities = {}\n",
        "        for label, _ in self.means.items():\n",
        "            # Start with the prior probability of the class\n",
        "            probabilities[label] = self.class_probabilities[label]\n",
        "            # Multiply by the probability of each feature\n",
        "            for i, feature in enumerate(input_features):\n",
        "                probabilities[label] *= self._calculate_probability(feature, self.means[label][i], self.stds[label][i])\n",
        "        # Return the class with the highest probability\n",
        "        return max(probabilities, key=probabilities.get)\n",
        "\n",
        "    def _calculate_probability(self, x, mean, std):\n",
        "        # Calculate the probability of a feature value with a Gaussian distribution\n",
        "        exponent = math.exp(-(math.pow(x-mean,2)/(2*math.pow(std,2))))\n",
        "        return (1 / (math.sqrt(2*math.pi) * std)) * exponent\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Predict a list of feature sets\n",
        "        return [self.predict_single(features) for features in X]\n",
        "\n",
        "    def classification_report(self, y_true, y_pred):\n",
        "        # Generate a classification report for the predictions\n",
        "        unique_labels = set(y_true)\n",
        "        report = {}\n",
        "        for label in unique_labels:\n",
        "            tp = sum(1 for i in range(len(y_true)) if y_true[i] == label and y_pred[i] == label)\n",
        "            fp = sum(1 for i in range(len(y_true)) if y_true[i] != label and y_pred[i] == label)\n",
        "            fn = sum(1 for i in range(len(y_true)) if y_true[i] == label and y_pred[i] != label)\n",
        "            tn = sum(1 for i in range(len(y_true)) if y_true[i] != label and y_pred[i] != label)\n",
        "\n",
        "            # Calculate precision, recall, and F1-score for each class\n",
        "            precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
        "            recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
        "            f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
        "            accuracy = (tp + tn) / len(y_true)\n",
        "\n",
        "            report[label] = {\n",
        "                'Precision': precision,\n",
        "                'Recall': recall,\n",
        "                'F1-score': f1,\n",
        "                'Accuracy': accuracy\n",
        "            }\n",
        "\n",
        "        return report"
      ],
      "metadata": {
        "id": "cHQabEM1Llbe"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Define the path to the CSV file containing the boston housing dataset\n",
        "    filepath = '/content/banana_quality.csv'\n",
        "\n",
        "    # Initialize the data handler with the filepath\n",
        "    # This object will handle all data operations\n",
        "    data_handler = DataHandler(filepath)\n",
        "\n",
        "    # Read the dataset from the CSV file using the read_csv method\n",
        "    # The dataset is returned as a list of lists, where each sublist is a row from the file\n",
        "    dataset = data_handler.read_csv()\n",
        "\n",
        "    # Split the dataset into training and testing parts using the train_test_split method\n",
        "    # Default split is 80% training and 20% testing\n",
        "    train_set, test_set = data_handler.train_test_split(dataset)\n",
        "\n",
        "    # Separate features and labels for the training set\n",
        "    # train_features will contain the data attributes, and train_labels will contain the target labels\n",
        "    train_features, train_labels = data_handler.separate_features_labels(train_set)\n",
        "\n",
        "    # Separate features and labels for the testing set\n",
        "    # This setup mirrors the training separation\n",
        "    test_features, test_labels = data_handler.separate_features_labels(test_set)\n",
        "\n",
        "    # Initialize the Naive Bayes Classifier\n",
        "    # This object will perform all classification tasks\n",
        "    classifier = NaiveBayesClassifier()\n",
        "\n",
        "    # Fit the classifier on the training data\n",
        "    # This process involves calculating necessary statistical parameters for the Naive Bayes algorithm\n",
        "    classifier.fit(train_features, train_labels)\n",
        "\n",
        "    # Predict the class labels for the test set features\n",
        "    # The predict method uses the trained model to estimate the labels of unseen data\n",
        "    predictions = classifier.predict(test_features)\n",
        "\n",
        "    # Generate a classification report comparing the true labels and predicted labels\n",
        "    # This report includes precision, recall, F1-score, and accuracy for each class\n",
        "    report = classifier.classification_report(test_labels, predictions)\n",
        "\n",
        "    # Print out the classification report for each class\n",
        "    print(\"Classification Report:\")\n",
        "    for label, metrics in report.items():\n",
        "        print(f\"Class {label}:\")\n",
        "        for metric, value in metrics.items():\n",
        "            print(f\"  {metric}: {value:.2f}\")\n",
        "        print()\n",
        "\n",
        "# This block checks if this script is the main program and runs the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkeTMHsILnLC",
        "outputId": "6c1fb2fa-1eb4-4027-c889-9e12d3350e6c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "Class Good:\n",
            "  Precision: 0.86\n",
            "  Recall: 0.85\n",
            "  F1-score: 0.86\n",
            "  Accuracy: 0.86\n",
            "\n",
            "Class Bad:\n",
            "  Precision: 0.85\n",
            "  Recall: 0.86\n",
            "  F1-score: 0.85\n",
            "  Accuracy: 0.86\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from random import shuffle\n",
        "from math import sqrt\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "y0Lhn7srR6Ol"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataHandler:\n",
        "    def __init__(self, filepath):\n",
        "        # Constructor to initialize the filepath\n",
        "        self.filepath = filepath\n",
        "\n",
        "    def read_csv(self):\n",
        "        # Read data from a CSV file and store it in a list\n",
        "        with open(self.filepath, 'r') as file:\n",
        "            csv_reader = csv.reader(file)\n",
        "            next(csv_reader)  # Skip the header row\n",
        "            dataset = [row for row in csv_reader]\n",
        "        return dataset\n",
        "\n",
        "    def train_test_split(self, dataset, test_size=0.2):\n",
        "        # Shuffle the dataset to ensure randomness\n",
        "        shuffle(dataset)\n",
        "        # Determine the split index based on the test size\n",
        "        split_index = int(len(dataset) * (1 - test_size))\n",
        "        # Split the dataset into training and testing sets\n",
        "        return dataset[:split_index], dataset[split_index:]\n",
        "\n",
        "    def separate_features_labels(self, dataset):\n",
        "        # Separate the features and labels from the dataset\n",
        "        # Convert the feature values to floats for computation\n",
        "        features = [list(map(float, data[1:-1])) for data in dataset]  # Exclude the ID and label\n",
        "        labels = [data[-1] for data in dataset]  # The label is the last element in each row\n",
        "        return features, labels"
      ],
      "metadata": {
        "id": "LzoDWwSESCGq"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class KNNClassifier:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        # KNN is a lazy learner; 'fit' just stores the data\n",
        "        self.X_train = np.array(X_train)\n",
        "        self.y_train = np.array(y_train)\n",
        "\n",
        "\n",
        "    def predict_single(self, input_features):\n",
        "        # Calculate the distance from the input features to all training data\n",
        "        distances = [np.sqrt(np.sum((x - input_features) ** 2)) for x in self.X_train]\n",
        "        # Get the indices of the k smallest distances\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "        # Find the most common class among these indices\n",
        "        k_labels = [self.y_train[i] for i in k_indices]\n",
        "        # Return the most common class\n",
        "        return max(set(k_labels), key=k_labels.count)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Predict a label for each feature set in X\n",
        "        return [self.predict_single(features) for features in X]\n",
        "\n",
        "    def classification_report(self, y_true, y_pred):\n",
        "        # Generate a classification report for the predictions\n",
        "        unique_labels = set(y_true)\n",
        "        report = {}\n",
        "        for label in unique_labels:\n",
        "            tp = sum(1 for i in range(len(y_true)) if y_true[i] == label and y_pred[i] == label)\n",
        "            fp = sum(1 for i in range(len(y_true)) if y_true[i] != label and y_pred[i] == label)\n",
        "            fn = sum(1 for i in range(len(y_true)) if y_true[i] == label and y_pred[i] != label)\n",
        "            tn = sum(1 for i in range(len(y_true)) if y_true[i] != label and y_pred[i] != label)\n",
        "\n",
        "            # Calculate precision, recall, F1-score, and accuracy for each class\n",
        "            precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
        "            recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
        "            f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
        "            accuracy = (tp + tn) / len(y_true)\n",
        "\n",
        "            report[label] = {\n",
        "                'Precision': precision,\n",
        "                'Recall': recall,\n",
        "                'F1-score': f1,\n",
        "                'Accuracy': accuracy\n",
        "            }\n",
        "        return report\n"
      ],
      "metadata": {
        "id": "1lOua6eONI5F"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "   # Define the path to the CSV file containing the boston housing dataset\n",
        "    filepath = '/content/banana_quality.csv'\n",
        "\n",
        "    # Initialize the data handler with the filepath\n",
        "    # This object will handle all data operations\n",
        "    data_handler = DataHandler(filepath)\n",
        "\n",
        "    # Initialize the data handler with the filepath\n",
        "    # This object will handle all data operations\n",
        "    dataset = data_handler.read_csv()\n",
        "\n",
        "\n",
        "    # Split the dataset into training and testing parts using the train_test_split method\n",
        "    # Default split is 80% training and 20% testing\n",
        "    train_set, test_set = data_handler.train_test_split(dataset)\n",
        "\n",
        "\n",
        "    # Separate features and labels for the training set\n",
        "    train_features, train_labels = data_handler.separate_features_labels(train_set)\n",
        "\n",
        "\n",
        "    # Separate features and labels for the testing set\n",
        "    # This setup mirrors the training separation\n",
        "    test_features, test_labels = data_handler.separate_features_labels(test_set)\n",
        "\n",
        "     # Initialize the KNN Classifier\n",
        "    # This object will perform all classification tasks\n",
        "    classifier = KNNClassifier(k=5)\n",
        "\n",
        "    # Fit the classifier on the training data\n",
        "    classifier.fit(train_features, train_labels)\n",
        "\n",
        "    # Predict the class labels for the test set features\n",
        "    predictions = classifier.predict(test_features)\n",
        "\n",
        "\n",
        "    # Generate a classification report comparing the true labels and predicted labels\n",
        "    # This report includes precision, recall, F1-score, and accuracy for each class\n",
        "    report = classifier.classification_report(test_labels, predictions)\n",
        "\n",
        "    print(\"Classification Report:\")\n",
        "    for label, metrics in report.items():\n",
        "        print(f\"Class {label}:\")\n",
        "        for metric, value in metrics.items():\n",
        "            print(f\"  {metric}: {value:.2f}\")\n",
        "        print()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "VncSVXR_SJ5u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}